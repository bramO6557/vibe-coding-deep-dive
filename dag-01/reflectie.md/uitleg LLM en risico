Een LLM is eigenlijk gewoon een slim computerprogramma dat heel veel tekst heeft gelezen Daardoor kan het zelf zinnen maken vragen beantwoorden of dingen uitleggen Het probeert steeds te raden welk woord logisch volgt en zo lijkt het alsof het snapt wat je bedoelt
Voor developers zit er wel een risico aan een LLM kan code geven die er supergoed uitziet maar toch fouten bevat Omdat het zo overtuigend klinkt denk je snel dat het klopt Als je dat zonder checken gebruikt kun je bugs of beveiligingsproblemen in je project krijgen